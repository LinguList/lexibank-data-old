{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LingPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lingpy import * # general settings\n",
    "from lingpy.basic.wordlist import get_wordlist # csv-to-wordlist converter\n",
    "from lingpy.evaluate.acd import bcubes # cognate detection evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that we want to load the CLDF-formatted file mixezoquean.csv. We have opened the terminal in the folder lexibank-data/cookbook, so we need to properly reference the path. We start by loading the CLDF-file as a simple LingPy-Wordlist. We specify the keyword \"row\" as \"parameter_name\", as this is the column in which we store the glosses for the concepts in CLDF. Likewise, we specify \"col\" as \"language_name\", since LingPy-Wordlists need to know where these columns are in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wl = get_wordlist(\"../datasets/mixezoquean/cldf/mixezoquean.csv\", row=\"parameter_name\", col=\"language_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the content of the wordlist file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wordlist has 1072 entries, 10 languages, 110 concepts, and 10 columns.\n"
     ]
    }
   ],
   "source": [
    "print(\"Wordlist has {0} entries, {1} languages, {2} concepts, and {3} columns.\".format(len(wl), wl.width, wl.height, len(wl.header)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we pass the wordlist object to the LexStat class. We specify the same parameters, but we pass an additional parameter \"segments\", to inform LingPy-LexStat where the segments are in the CLDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lex = LexStat(wl, col='language_name', row=\"parameter_name\", segments=\"segments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now carry out a quick cognate detection analysis, using LexStat's \"lexstat\" function. We set the keyword \"ref\" to \"lexstat\" to indicate in which column the automatic cognate detection should be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++|\n",
      "|++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++|\n",
      "|++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++|\n",
      "LexStat object has {'duplicates': 17, 'weights': 15, 'language_iso': 3, 'language_name': 2, 'cognacy': 9, 'langid': 13, 'parameter_name': 5, 'prostrings': 11, 'lexstat': 18, 'segments': 8, 'id': 0, 'ipa': 16, 'numbers': 14, 'language_id': 1, 'sonars': 10, 'source': 7, 'parameter_id': 4, 'value': 6, 'classes': 12} columns.\n"
     ]
    }
   ],
   "source": [
    "lex.get_scorer(runs=1000)\n",
    "lex.cluster(method=\"lexstat\", ref=\"lexstat\", threshold=0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test how well the automatic cognate detection performed, by comparing the content in the column \"cognacy\" (default name for cognate sets in CLDF) with the content in the column \"lexstat\", using LingPy's bcubes-function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************')\n",
      "* B-Cubed-Scores        *\n",
      "* --------------------- *\n",
      "* Precision:     0.9490 *\n",
      "* Recall:        0.8502 *\n",
      "* F-Scores:      0.8969 *\n",
      "*************************'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9489894278606967, 0.8501536816784715, 0.8968567888703056)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcubes(lex, 'cognacy', 'lexstat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, our precision is quite high, which is good, as it means there are not many false positives. Recall could be improved, but we should be happy with almost 90%, given the small size of the wordlist."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
